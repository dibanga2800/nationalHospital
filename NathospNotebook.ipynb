{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db91701d-25eb-45fe-a73d-c957b2c003b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ing storage account"
    }
   },
   "outputs": [],
   "source": [
    "## mounting storage account\n",
    "# Variables\n",
    "storage_account_name = \"nationalhospitaldatalake\"\n",
    "container_name = \"nationalhospital\"\n",
    "storage_account_key = dbutils.secrets.get(\n",
    "    scope=\"datalakekey\",\n",
    "    key=\"datalakekey\"\n",
    ")\n",
    "\n",
    "# Mount Point\n",
    "mount_point = f\"/mnt/{container_name}\"\n",
    "\n",
    "# Mount the storage account\n",
    "dbutils.fs.mount(\n",
    "    source=f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net\",\n",
    "    mount_point=mount_point,\n",
    "    extra_configs={\n",
    "        f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": storage_account_key }\n",
    ")\n",
    "\n",
    "print(f\"Mounted {mount_point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee26a129-6cfc-4789-a7e5-ba32ea9116a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Data Reading\n",
    "patients_df = spark.read.csv(f\"{mount_point}/raw/patients_data.csv\", header=True, inferSchema=True, quote='\"', escape='\"', multiLine=True)\n",
    "\n",
    "imaging_df = spark.read.csv(f\"{mount_point}/raw/imaging_results_data.csv\", header=True, inferSchema=True, quote='\"', escape='\"', multiLine=True)\n",
    "\n",
    "lab_df = spark.read.csv(f\"{mount_point}/raw/lab_results_data.csv\", header=True, inferSchema=True, quote='\"', escape='\"', multiLine=True)\n",
    "\n",
    "med_records_df = spark.read.csv(f\"{mount_point}/raw/medical_records_data.csv\", header=True, inferSchema=True, quote='\"', escape='\"', multiLine=True)\n",
    "\n",
    "trials_df = spark.read.csv(f\"{mount_point}/raw/clinical_trials_data.csv\", header=True, inferSchema=True, quote='\"', escape='\"', multiLine=True)\n",
    "\n",
    "participants_df = spark.read.csv(f\"{mount_point}/raw/trial_participants_data.csv\", header=True, inferSchema=True, quote='\"', escape='\"', multiLine=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9864883e-735c-4b6a-bb27-5139daad4ba2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Cleaning and Transformation\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "##patients data\n",
    "processed_patients_df = patients_df.dropDuplicates() \\\n",
    "    .na.drop() \\\n",
    "    .withColumn(\"loaded_date\", current_date()) \\\n",
    "    .write.mode(\"overwrite\").csv(f\"{mount_point}/processed_data/patients_data_processed\",\n",
    "    header=True)\n",
    "\n",
    "##medical record data\n",
    "processed_med_records_df = med_records_df.dropDuplicates() \\\n",
    "    .na.drop() \\\n",
    "    .withColumn(\"loaded_date\", current_date()) \\\n",
    "    .write.mode(\"overwrite\").csv(f\"{mount_point}/processed_data/med_records_data_processed\",\n",
    "    header=True)\n",
    "\n",
    "##imaging data\n",
    "processed_imaging_df = imaging_df.dropDuplicates() \\\n",
    "    .na.drop() \\\n",
    "    .withColumn(\"loaded_date\", current_date()) \\\n",
    "    .write.mode(\"overwrite\").csv(f\"{mount_point}/processed_data/imaging_data_processed\",\n",
    "    header=True)\n",
    "\n",
    "\n",
    "##lab data\n",
    "processed_lab_df = lab_df.dropDuplicates() \\\n",
    "        .na.drop() \\\n",
    "    .withColumn(\"loaded_date\", current_date()) \\\n",
    "    .write.mode(\"overwrite\").csv(f\"{mount_point}/processed_data/lab_data_processed\",\n",
    "    header=True)\n",
    "\n",
    "##clinical trials data\n",
    "processed_trials_df = trials_df.dropDuplicates() \\\n",
    "    .na.drop() \\\n",
    "    .withColumn(\"loaded_date\", current_date()) \\\n",
    "    .write.mode(\"overwrite\").csv(f\"{mount_point}/processed_data/trials_data_processed\",\n",
    "    header=True)\n",
    "\n",
    "##participants data\n",
    "processed_participants_df = participants_df.dropDuplicates() \\\n",
    "    .na.drop() \\\n",
    "    .withColumn(\"loaded_date\", current_date()) \\\n",
    "    .write.mode(\"overwrite\").csv(f\"{mount_point}/processed_data/participants_data_processed\",\n",
    "    header=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1fe8e3f-d51a-4368-af0d-b505966e5e75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#unmount the storage\n",
    "dbutils.fs.unmount(mount_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32869235-700d-422f-95cd-1462eab680c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## This was a fantastic project with databricks"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NathospNotebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
